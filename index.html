<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title> Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models </title>
  <link rel="icon" type="image/x-icon" href="static/images/watermark.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zwhe99.github.io/" target="_blank">Zhiwei He</a><sup>*,1</sup>,</span>
                <span class="author-block">
                  <a href="https://github.com/Zhou-bl" target="_blank">Binglin Zhou</a><sup>*,1</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=a3UulbMAAAAJ&hl=zh-CN" target="_blank">Hongkun Hao</a><sup>1</sup>,
                    <span class="author-block">
                      <a href="https://exlaw.github.io/" target="_blank">Aiwei Liu</a><sup>2</sup>,
                      <span class="author-block">
                        <a href="http://xingwang4nlp.com/" target="_blank">Xing Wang</a><sup>3</sup>,
                        <span class="author-block">
                          <a href="http://www.zptu.net/" target="_blank">Zhaopeng Tu</a><sup>3</sup>,
                          <span class="author-block">
                            <a href="https://bcmi.sjtu.edu.cn/home/zhangzs/" target="_blank">Zhuosheng Zhang</a><sup>1</sup>,
                            <span class="author-block">
                              <a href="https://wangruinlp.github.io/" target="_blank">Rui Wang</a><sup>†,1</sup>
                            </span>
                        </span>
                      </span>
                    </span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>Shanghai Jiao Tong University, <sup>2</sup>Tsinghua University, <sup>3</sup>Tencent AI Lab
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution, <sup>†</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2402.14007.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <span class="link-block">
                    <a href="https://github.com/zwhe99/X-SIR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.14007" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of "cross-lingual consistency" in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-normal">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Cross-lingual Consistency of Watermark</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="Text" style="text-align: left; font-size: larger;">
            In this section, we test the cross-lingual consistency of text watermarking. 
            The text watermark algorithm we use is KGW, UW and SIR. 
            We use English prompt as the input of the large language model, and then translate the output into different languages.
            In this work, we translate the output into Chinese, French, German and Janpanese using gpt-3.5-turbo.
            <a href="#Fig1">Fig1</a> shows the changes of watermark strength score after translation.
            Experimental results show that KGW, UW and SIR lack cross-lingual consistency.
            <div class="item">
              <!-- Your image here -->
              <a name="Fig1">
                <img src="static/images/Fig1.png" alt="Cross-lingual Consistency of Text Watermark" style="margin: 0 auto;"/>
              </a>
              <h2 class="subtitle has-text-centered" style="font-size: medium;">
                Fig1: Watermark strength score (detection score) after translation.
              </h2>
            </div>
            <class="Text" style="text-align: left; font-size: large;">
              <a href="#Fig2">Fig2</a> shows the <b>PCC</b> (Pearson Correlation Coefficient) and <b>RE</b> (Relative Error) of the watermark strength score after translation.
              The <b>PCC</b> results show that the similar language pairs (En&Fr, En&De) have higher <b>PCC</b> values than the dissimilar language pairs (En&Zh, En&Ja).
              While the <b>RE</b> results show that <b>SIR</b> reserves the watermark strength score better than <b>UW</b> and <b>KGW</b> after translation.
              In another word, <b>SIR</b> has better cross-lingual consistency than <b>UW</b> and <b>KGW</b>.
            </div>
              
            <div class="item">
              <!-- Your image here -->
              <a name="Fig2">
                <img src="static/images/Fig2.png" alt="PCC and RE result of translated text"/>
              </a>
              <h2 class="subtitle has-text-centered" style="font-size: medium;">
                Fig2: <b>PCC</b> and <b>RE</b> of the watermark strength score after translation.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </div>
    
    
</section>



<section class="hero is-normal">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Cross-lingual Watermark Removal Attack (CWRA)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="Text" style="text-align: left; font-size: larger;">
            In the previous section, we focus on scenarios where the response of LLM is translated into other languages.
            However, an attacker typically expects a response from the LLM in the same language as the prompt while removing watermarks.
            To bridge this gap, we introduce the Cross-lingual Watermark Removal Attack (CWRA) in this section, constituting a complete attack process and posing a more significant challenge to text watermarking than paraphrasing and re-translation attacks.
            <div class="Text" style="text-align: left; font-size: large;">
              <a ref="#Fig3">Fig3</a> shows the process of CWRA.
              Instead of feeding the original prompt into the LLM, the attacker initiates the attack by translating the prompt into a pivot language named the pivot prompt.
              The LLM receives the pivot prompt and provides a watermarked response in the pivot language.
              The attacker then translates the pivot response back into the original language.
              This approach allows the attacker to obtain the response in the original language.
              Due to the inherent challenges in maintaining cross-lingual consistency, the watermark would be effectively eliminated during the second translation step.
            </div>
            <div class="item">
              <!-- Your image here -->
              <a name="Fig3">
                <img src="static/images/Fig3.png" alt="CWRA pipeline" style="margin: 0 auto;"/>
              </a>
              <h2 class="subtitle has-text-centered" style="font-size: medium;">
                Fig3: The pipeline of CWRA. We select Chinese as the pivot language as an example.
              </h2>
            </div>
            <div class="Text" style="text-align: left; font-size: large;">
              To assess the practicality of attack methods, we consider two downstream tasks: text summarization and question answering.
              <a href="#Fig4">Fig4</a> shows the ROC curves of the watermark strength score under different attack methods.
            </div>
              
            <div class="item">
              <!-- Your image here -->
              <a name="Fig4">
                <img src="static/images/Fig4.png" alt="AUC of the watermark strength score under different attack methods."/>
              </a>
              <h2 class="subtitle has-text-centered" style="font-size: medium;">
                Fig4: ROC curves of the watermark strength score under different attack methods.
              </h2>
            </div>
            <div class="Text" style="text-align: left; font-size: large;">
              <a href="#Fig5">Fig5</a> shows the text quality under different attack methods.
            </div>
            <div class="item">
              <!-- Your image here -->
              <a name="Fig5">
                <img src="static/images/Fig5.png" alt="Text quality"/>
              </a>
              <h2 class="subtitle has-text-centered" style="font-size: medium;">
                Fig5: Text quality under different attack methods.
              </h2>

              <div class="Text" style="text-align: left; font-size: large;">
                Among the compared methods, CWRA stands out for its superior performance.
                Considering that the same translator and paraphraser were used across all methods, we speculate that this is because the <b>Baichuan-7B</b> model used in our experiments performs even better in the pivot language (Chinese) than in the original language (English).
                This finding implies that a potential attacker could strategically choose a pivot language at which the LLM excels to perform CWRA, thereby achieving the best text quality while removing the watermark.
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
    
</section>

<section class="hero is-normal">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Improving Cross-lingual Consistency</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="Text" style="text-align: left; font-size: larger;">
            Up to this point, we have observed the challenges associated with text watermarking in cross-lingual scenarios.
            In this section, we first analyze two key factors essential for achieving cross-lingual consistency.
            Based on our analysis, we propose a defense method against CWRA.
            <div class="Text" style="text-align: left; font-size: large;">
              KGW-based watermarking methods fundamentally depend on the partition of the vocabulary.
              Therefore, cross-lingual consistency aims to ensure that the green tokens in the watermarked text will still be recognized as green tokens after being translated into other languages.
              We analyze the two key factors that contribute to the cross-lingual consistency in text watermarking:
              <ul>
                <li><b>Factor 1</b>: <i>Cross-lingual semantic clustering of the vocabulary.</i> Semantically similar tokens must be in the same partition, either green or red lists. </li>
                <li><b>Factor 2</b>: <i>Cross-lingual semantic robust vocabulary partition.</i> For semantically similar prefixes in different languages: <b>I watch</b> and <b>我 看</b>, the partitions of the vocabulary are the same. </li>
              </ul>
            </div>
            <div class="item">
              <!-- Your image here -->
              <a name="Fig6">
                <img src="static/images/Fig6.png" alt="CWRA pipeline" style="margin: 0 auto;"/>
              </a>
            </div>
            <div class="Text" style="text-align: left; font-size: large;">
              To satisfy the factor 2: we can use a multilingual embedding model to obtain the similar embeddings of the tokens in different languages with similar semantic.
              Then to satisfy the factor 1, we introduce a cross-lingual semantic clustering of the vocabulary.
              We split the vocabulary into different clusters: in each cluster, the tokens are semantically similar (even in different languages).
              For example, the token <b>watch</b> and <b>看</b> are in the same cluster.
              Then for the tokens in the same cluster, we add the same bias to their logits when generating the watermarked text.
              More details can be found in our paper.
              We name the proposed method as <b>X-SIR</b>.
              The <a href="#Fig7">Fig7</a> and <a href="#Fig8">Fig8</a> show the performance of <b>X-SIR</b> under CWRA.
              <div class="item">
                <!-- Your image here -->
                <table><tr>
                  <td><a name="Fig7">
                    <img src="static/images/Fig7.png" alt="AUC of the watermark strength score under different attack methods."/>
                  </a>
                  <h2 class="subtitle has-text-centered" style="font-size: medium;">
                    Fig7: ROC curves of X-SIR and SIR under CWRA.
                  </h2>
                  </td>
                  <td><a name="Fig8">
                    <img src="static/images/Fig8.png" alt="Text quality"/>
                  </a>
                  <h2 class="subtitle has-text-centered" style="font-size: medium;">
                    Fig8: Text quality of X-SIR and SIR.
                  </h2>
                  </td>
                </tr>
                </table>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    
</section>

<section class="hero is-normal is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Conclusion</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="Text" style="text-align: left; font-size: larger;">
            This work aims to investigate the cross-lingual consistency of watermarking methods for LLMs.
            We first characterize and evaluate the cross-lingual consistency of current watermarking techniques for LLMs, revealing that current watermarking methods struggle to maintain their watermark strengths across different languages.
            Based on this observation, we propose the cross-lingual watermark removal attack (CWRA), which significantly challenges watermark robustness by efficiently eliminating watermarks without compromising performance.
            Through the analysis of two primary factors that influence cross-lingual consistency, we propose X-SIR as a defense strategy against CWRA.
            Despite its limitations, this approach greatly improves the AUC and paves the way for future research.
            Overall, this work completes a closed loop in the study of cross-lingual consistency in watermarking, including: evaluation, attacking, analysis, and defensing.
          </div>
        </div>
      </div>
    </div>
    
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{he2024watermarks,
        title={Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models}, 
        author={Zhiwei He and Binglin Zhou and Hongkun Hao and Aiwei Liu and Xing Wang and Zhaopeng Tu and Zhuosheng Zhang and Rui Wang},
        year={2024},
        eprint={2402.14007},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
